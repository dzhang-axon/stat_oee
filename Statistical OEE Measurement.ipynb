{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scipy\n\nfrom ipywidgets import interactive, widgets\nfrom IPython.display import display\n\nfrom IPython.utils import io","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Statistical OEE Measurement</b>\n\nOverall Equipment Effectiveness (OEE) is the product of Availability (equipment uptime), Productivity (cycle time or capacity), and Quality (process yield) as compared to reference values. The reference values are typically taken from the Systems Requirements Specification (SRS) or project charter. At nominal availability, productivity, and quality, OEE is 1 (equivalently expressed as 100%). \n\nWhen validating a system or machine for Site Acceptance Testing (SAT), the system is run for a fixed duration to obtain sample data. In theory, the sample data can be statistically analyzed to produce a confidence interval for the true OEE of the system, which can then be compared against target OEE values to determine whether the system passes SAT. In reality, while Quality and Productivity may be analyzed in this way, the sensitivity of Availability to infrequent events and its correlation to long-term, seasonal variability makes Availability extremely difficult to model statistically, especially in the context of SAT where historical data are not available. The difficulties of predicting Availability are discussed in further detail at the end of this article, but a statistical solution is not presented. \n\nWith regards to Quality and Productivity and the statistical analysis of SAT, more data provides greater confidence in SAT results, but more data requires more time and resources to collect. Thus, maximizing SAT result confidence and minimizing required SAT time are competing requirements. This article seeks to establish a quantitative relationship between target confidence level and the amount of data required, which should allow SAT planners to control OEE measurement accuracy while optimizing SAT duration. "},{"metadata":{},"cell_type":"markdown","source":"<b>Measuring Quality</b>\n\nQuality can be modeled as a binomial random variable (pass or fail), and the confidence interval can be given by tables or approximated using the Wilson score interval as follows:\n$$\n\\begin{aligned}\np \\approx \\frac{1}{n + z^2}\\left( s+\\frac{1}{2}z^2 \\pm z \\sqrt{\\frac{sf}{n}+\\frac{z^2}{4}} \\right)\n\\end{aligned}\n$$\nwhere $+p$ and $-p$ are the upper and lower bounds of the confidence interval, respectively; $n$ is the sample size; $s$ is the number of observed successes; $f$ is the number of observed failures (such that $f = n - s$); and $z$ is the probit, i.e. the quantile of the normal distribution corresponding to a confidence level $c$. For example, for a 95% confidence level, $z \\approx 1.96$. \n\nAn interactive calculator is implemented below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# wilson score interval\nfrom scipy import stats\nfrom scipy.stats import norm\n\ndef wilson_ci(c=0.95, m=0.95, n=1000):\n    (s, f) = (m*n, n - m*n) # successes, failures\n    z = norm.ppf(1 - 0.5*(1-c)) # normal probit\n\n    p = (((s + 0.5*z**2) - z*((s*f)/n + z**2/4)**0.5)/(n + z**2), ((s + 0.5*z**2) + z*((s*f)/n + z**2/4)**0.5)/(n + z**2))\n\n    print(\"The %2d%% confidence interval is (%4.3f, %4.3f).\" % (c*100, p[0], p[1]))\n    print(\"The size of the confidence interval is %4.3f.\" % (p[1]-p[0]))\n    \n    return p","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w = interactive(\n    wilson_ci, \n    c = widgets.FloatSlider(min=0.70, max=0.99, value=0.95, step=0.01, description=\"confidence level\"),\n    m = widgets.FloatSlider(min=0.50, max=1.00, value=0.95, step=0.01, description=\"sample yield\"), \n    n = widgets.IntSlider(min=50, max=5000, value=1000, step=50, description = \"sample size\"))\nw","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(FloatSlider(value=0.95, description='confidence level', max=0.99, min=0.7, step=0.01), F…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b102ae3ac8148debcd3f5428d3b5c30"}},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"As shown by the above demonstration, increasing the sample size reduces measurement uncertainty (decreases the size of the confidence interval). Also, increasing the sample yield reduces measurement uncertainty (since yield is known to be bounded at 100%), whereas moving the sample yield toward 50% increases the measurement uncertainty. Thus, to maximize confidence when measuring Quality, the sample size should be maximized, especially if the yield is low. If the sample yield is very high (99% or greater), a lower sample size can achieve the same level of confidence. "},{"metadata":{},"cell_type":"markdown","source":"<b>Measuring Productivity</b>\n\nProductivity can be quantified as a function of cycle time, and cycle time can be modeled as a normally-distributed random variable. Thus, the confidence interval can be calculated using the Student t distribution as follows.\n$$\n\\begin{aligned}\np = \\mu \\pm \\frac{\\alpha\\sigma}{2\\sqrt{n}}\n\\end{aligned}\n$$\nwhere $+p$ and $-p$ are the upper and lower bounds of the confidence interval, respectively; $\\alpha$ is the one-sided target error (i.e. $\\alpha = \\frac{1+c}{2}$ where $c$ is the confidence level); $n$ is the sample size; $\\mu$ is the sample mean; and $\\sigma$ is the sample standard deviation, i.e. $\\sigma^2$ is the sample variance. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# student t confidence interval\ndef norm_ci(c=0.95, ct=2.0, m=2.0, s=0.4, n=1000):\n    a = 1 - 0.5*(1-c) # one-sided error\n\n    p = (m - a*s/(n**0.5), m + a*s/(n**0.5))\n\n    print(\"The %2d%% confidence interval is (%4.3f, %4.3f).\" % (c*100, ct/p[1], ct/p[0]))\n    print(\"The size of the confidence interval is %4.3f.\" % (ct/p[0] - ct/p[1]))\n    \n    return (ct/p[1], ct/p[0])","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z = interactive(\n    norm_ci, \n    c = widgets.FloatSlider(min=0.70, max=0.99, value=0.95, step=0.01, description=\"confidence level\"),\n    ct = widgets.FloatSlider(min=0.5, max=5.0, value=2.0, step=0.1, description=\"target CT\"),\n    m = widgets.FloatSlider(min=0.5, max=5.0, value=2.0, step=0.1, description=\"sample CT\"), \n    s = widgets.FloatSlider(min=0.1, max=1.0, value=0.4, step=0.1, description=\"sample stdev\"),\n    n = widgets.IntSlider(min=50, max=5000, value=1000, step=50, description = \"sample size\"))\nz","execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(FloatSlider(value=0.95, description='confidence level', max=0.99, min=0.7, step=0.01), F…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"583602380f10404187b18f6ed8612b22"}},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Just as with the Quality measurement, increasing the sample size reduces measurement uncertainty of the Productivity measurment as well. Decreasing the sample standard deviation also decreases the measurement uncertainty, although this is a property of the process and is not directly controllable during the SAT setup. Note that because Productivity compares cycle time against a target value (typically defined in the SRS), if the measured CT outperforms the target CT, the Productivity parameter may exceed 1. \n\nNote also that this model is an approximation with known limitations. For instance, the cycle time cannot be negative. When the sample CT is large and the sample $\\sigma$ is small, then this is a good approximation. However, if $\\mu - 2\\sigma \\leq 0$, then the cycle time distribution is most likely not normal (specifically, left skewed) and the above results will be invalid. For an automated process, this should be uncommon. "},{"metadata":{},"cell_type":"markdown","source":"<b>Measuring Availability</b>\n\nAvailability is the hardest metric to measure. Availability can be understood as a time average of machine status, where the machine status takes a boolean value (\"operational/up\" or \"unoperational/down\"). Availablity can be estimated using Reliability Block Diagrams or Fault Tree Analysis, but both are highly dependent on subjective brainstorming and specific expertise, and are not strictly empirical methods. Empirical methods for forecasting these types of infrequent events is difficult in general (e.g. research on using a two-state Markov-chain to model the Clear Sky Index for solar irradiance), and impossible without abundant historical data; thus, forecasting and/or measuring of equipment availability during a SAT run of a new machine is impossible. \n\nThe impossibility of measuring availability using a SAT run can be illustrated as follows. Let us assume that failure events follow a Poisson distribution (memoryless). As a general property, if a Poisson event has a mean frequency of 1 over a given period, the probability that the event never occurs over that same period is roughly 37%. Applied to our example, if we expect the machine to fail on average once per day, and we perform a SAT run which lasts one day, then there is a 37% chance that no failure occurs during the SAT run. From the opposite perspective, if no failure occurs during the SAT run, one can conclude with at most 63% confidence that the long-term failure rate will be less than once per day. This is dramatically worse than the typical 95% confidence standard applied to general statistical inferences. If the SAT run is shorter, say 5 hours, or half a shift, then a failure-free SAT run would give at most 63% confidence that the long-term failure rate will be less than once per 5 hours; in other words, even if the system demonstrates a perfect 5-hour run, there is still at least 37% percent chance that, in the long-run, the system will fail more than once per 5 hours! \n\nFurthermore, measuring availability requires not only the measurement of failure frequency, but also of the Mean Time To Repair (MTTR). Even if the TTR of failure events is a normally distributed random variable (a na&iuml;ve assumption), a statistical sample of at least 30 failures must be observed to make a meaningful measurement. This is obviously not practical during SAT. \n\nIn conclusion, while Availability can be a significant contributor to OEE, it cannot be empirically measured during a SAT run. Therefore, alternative methods such as Fault Tree Analysis or a simple aggregate estimate must be employed to obtain a realistic result for the system's OEE. However, one must be careful about choosing Availability definitions when factoring Availability into SAT performance. Overall Availability is defined simply as downtime over total time, where downtime includes unscheduled maintenance, scheduled (preventive) maintenance, schedule loss, startup and shift changes, etc. Inherent Availability includes only downtime caused by unscheduled maintenance, and does not include downtime that is not caused directly by the system itself. When assessing a system for SAT, Inherent Availability should be the relevant parameter, but one must note that the Overall Availability observed over long-term usage of the system will necessarily be lower than the Inherent Availability, and how this impacts factory planning must be further scrutinized. "},{"metadata":{},"cell_type":"markdown","source":"<b>Measuring OEE</b>\n\nAggregating the above results gives our statistical OEE measurement. As explained earlier, Availability will not be included in this aggregate, but should be taken into account using other means. Any Availability estimate should then be used to derate the OEE presented here to give a more realistic OEE result. \n\nTaking a partial (excludes Availability) OEE as\n$$\n\\begin{aligned}\nOEE_{Q,P} = Quality \\times Performance\n\\end{aligned}\n$$\nthe result is as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"c = w.kwargs[\"c\"]\nn = w.kwargs[\"n\"]\n\ndef oee_ci(c=w.kwargs[\"c\"], n=w.kwargs[\"n\"]):\n    with io.capture_output() as captured:\n        p_Q = wilson_ci(c, w.kwargs[\"m\"], n)\n    with io.capture_output() as captured:\n        p_P = norm_ci(c, z.kwargs[\"ct\"], z.kwargs[\"m\"], z.kwargs[\"s\"], n)\n    \n    p = (p_Q[0] * p_P[0], p_Q[1] * p_P[1])\n    print(\"The OEE %2d%% confidence interval is (%4.3f, %4.3f).\" % (c*100, p[0], p[1]))\n    print(\"The size of the OEE confidence interval is %4.3f.\" % (p[1] - p[0]))\n    \n    return p\n\noee = interactive(\n    oee_ci, \n    c = widgets.FloatSlider(min=0.70, max=0.99, value=w.kwargs[\"c\"], step=0.01, description=\"confidence level\"),\n    n = widgets.IntSlider(min=50, max=5000, value=w.kwargs[\"n\"], step=50, description = \"sample size\"))\noee","execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(FloatSlider(value=0.95, description='confidence level', max=0.99, min=0.7, step=0.01), I…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c8fa737afa74cd1b59f91eba44cb659"}},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"NB: The above result uses the sample aggregate data (mean, standard deviation, etc.) from the above sections. To change those data, the values must be changed in the above sliders. \n\nIf the lower bound $-p$ of the confidence interval meets the SAT requirement, then there is at least a $\\frac{1-c}{2}$ chance that the true OEE meets the SAT requirement, where $c$ is the confidence level. If the lower bound fails the SAT requirement, then the confidence level can be adjusted in the above sliders until the lower bound rises above the SAT requirement. The chance that the true OEE meets the SAT requirement can then be calculated (as $\\frac{1-c}{2}$) using this lower confidence level. \n\nAs shown in this article, the required confidence level depends upon the performance of the system during SAT. If the system significantly exceeds expectations, then the confidence level required to ensure conformity is lower. On the other hand, if the system performance during SAT hovers around or just barely exceeds SAT requirements, then the confidence level required to ensure conformity will be very high. In this latter case, a short SAT run may not generate enough data to meet the required confidence level, so the SAT run may be extended, or a follow-up SAT run may be performed to generate more data and increase the confidence level. Thus, this article presents a framework for dynamically adjusting the length of the SAT run to ensure the system performs to specification. "},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"}},"nbformat":4,"nbformat_minor":4}